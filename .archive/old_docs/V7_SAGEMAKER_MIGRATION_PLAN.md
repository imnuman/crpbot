# V7 Model Training: SageMaker Migration Plan

## Executive Summary

**Status**: V6 and V7 models are **both broken** due to missing feature normalization.
**Solution**: Retrain V7 on **AWS SageMaker** with guaranteed StandardScaler integration.
**Timeline**: 6-9 hours for all 3 models
**Cost**: ~$12-14 total

---

## Critical Findings from V7 Diagnostic

### V7 Models Failed All Quality Gates âŒ

| Model | Logit Range | Scaler Present | Status |
|-------|-------------|----------------|--------|
| **BTC-USD** | Â±158,041 | âŒ No | **WORSE than V6** |
| **ETH-USD** | Â±4,759 | âŒ No | Failed |
| **SOL-USD** | Â±500 | âŒ No | Failed |

**Target**: Logit range â‰¤20 (current: up to 158,000!)

### Root Cause

Despite V7 training plan calling for StandardScaler normalization:
- âœ… Architecture has dropout, batch norm, temperature scaling
- âŒ **StandardScaler NOT saved in checkpoint**
- âŒ **Raw features (79,568 for BTC) fed directly to network**
- âŒ **No normalization applied during training**

**Result**: Same extreme logits and fake 100% confidence as V6.

---

## V6 vs V7 Comparison (Actual vs Planned)

| Feature | V6 Enhanced | V7 (Actual) | V7 (Planned) |
|---------|-------------|-------------|--------------|
| **Dropout** | âŒ | âœ… 0.3 | âœ… 0.3 |
| **Batch Norm** | âŒ | âœ… | âœ… |
| **Temperature** | âŒ | âœ… 2.5 | âœ… 2.5 |
| **StandardScaler** | âŒ | âŒ **MISSING!** | âœ… Required |
| **Scaler in Checkpoint** | âŒ | âŒ **MISSING!** | âœ… Required |
| **Logit Range** | Â±40,000 | Â±158,000 | Â±10 |
| **Confidence** | Fake 100% | Fake 100% | Real 60-85% |

**Conclusion**: V7 training partially implemented - architecture improvements present, but critical normalization missing.

---

## Why SageMaker?

### Current Issues with Manual Training

1. **No Verification**: Training scripts don't verify scaler is saved
2. **Inconsistent Environment**: Different CUDA versions, PyTorch versions
3. **Manual Monitoring**: No automatic early stopping or checkpointing
4. **Cost Inefficiency**: GPU instance left running after training

### SageMaker Advantages

âœ… **Managed Infrastructure**: Automatic instance provisioning and cleanup
âœ… **Built-in Monitoring**: CloudWatch logs, TensorBoard integration
âœ… **Reproducible**: Containerized environment, version-controlled
âœ… **Cost Efficient**: Pay only for training time, auto-shutdown
âœ… **S3 Integration**: Direct model artifact upload
âœ… **Hyperparameter Tuning**: Automatic optimization (future)

---

## SageMaker Training Architecture

### Critical Normalization Flow

```python
# STEP 1: Fit scaler on training data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)

# STEP 2: Pass scaler to model during initialization
model = V7EnhancedFNN(input_size=72, scaler=scaler)

# STEP 3: Model's forward() applies normalization
def forward(self, x):
    if self.scaler is not None:
        x = self.scaler.transform(x)  # â† Applied here
    # ... rest of forward pass

# STEP 4: Save scaler in checkpoint
checkpoint = {
    'model_state_dict': model.state_dict(),
    'scaler': scaler,  # â† CRITICAL: Must be here!
    'scaler_mean': scaler.mean_,
    'scaler_scale': scaler.scale_
}

# STEP 5: Verify scaler is saved
loaded = torch.load(checkpoint_path)
assert 'scaler' in loaded, "CRITICAL ERROR!"
```

### Training Pipeline

```
S3 Input                     SageMaker Training Instance (ml.g5.xlarge)
â”€â”€â”€â”€â”€â”€â”€â”€                     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

data/features/               1. Load features from S3
â”œâ”€ BTC-USD.parquet    â”€â”€â”€â”€â†’  2. Fit StandardScaler on train set
â”œâ”€ ETH-USD.parquet           3. Initialize model WITH scaler
â””â”€ SOL-USD.parquet           4. Train with normalized features
                             5. Save checkpoint WITH scaler
code/                        6. Verify scaler in checkpoint
â””â”€ sagemaker_train.py â”€â”€â”€â”€â†’  7. Run diagnostic (logits <20?)
                             8. Upload to S3 if passed
                                    â”‚
                                    â–¼
                             S3 Output
                             â”€â”€â”€â”€â”€â”€â”€â”€â”€
                             models/
                             â”œâ”€ BTC-USD_v7.pt (with scaler)
                             â”œâ”€ ETH-USD_v7.pt (with scaler)
                             â””â”€ SOL-USD_v7.pt (with scaler)
```

---

## Training Configuration

### Instance: ml.g5.xlarge

- **GPU**: NVIDIA A10G (24GB VRAM)
- **vCPUs**: 4
- **RAM**: 16GB
- **Cost**: $1.41/hour (on-demand)
- **Region**: us-east-1

### Hyperparameters

```python
{
    'epochs': 30,
    'batch_size': 64,
    'learning_rate': 0.001,
    'dropout': 0.3,
    'temperature': 2.5,
    'label_smoothing': 0.05,
    'focal_alpha': 0.25,
    'focal_gamma': 2.0,
    'early_stopping_patience': 5
}
```

### Quality Gates (Automatic Verification)

After each epoch, check:
- âœ… Scaler present in checkpoint
- âœ… Logit range <20 on validation set
- âœ… Overconfidence (<99%) <10%
- âœ… No class >60% of predictions

**Training auto-stops** if gates fail for 3 consecutive epochs.

---

## Step-by-Step Execution Plan

### Phase 1: Setup (30 minutes)

1. **Create S3 bucket**: `s3://crpbot-sagemaker-training`
2. **Upload training data**:
   ```bash
   aws s3 cp data/features/ s3://crpbot-sagemaker-training/data/features/ --recursive
   ```
3. **Upload training scripts**:
   ```bash
   aws s3 cp sagemaker_train.py s3://crpbot-sagemaker-training/code/
   aws s3 cp apps/trainer/amazon_q_features.py s3://crpbot-sagemaker-training/code/
   ```
4. **Create IAM role** for SageMaker execution

### Phase 2: Training (6-9 hours)

**Train all 3 models in parallel** using SageMaker:

```python
# BTC-USD Training Job
btc_estimator = PyTorch(
    entry_point='sagemaker_train.py',
    role=sagemaker_role,
    instance_type='ml.g5.xlarge',
    instance_count=1,
    framework_version='2.1.0',
    py_version='py310',
    hyperparameters={'symbol': 'BTC-USD', 'epochs': 30},
    output_path='s3://crpbot-sagemaker-training/models/'
)
btc_estimator.fit({'training': 's3://crpbot-sagemaker-training/data/features/'})

# Repeat for ETH-USD and SOL-USD
```

**Monitoring**:
- CloudWatch Logs: `/aws/sagemaker/TrainingJobs`
- Metrics: Loss, accuracy, confidence distribution
- Automatic early stopping if quality gates fail

### Phase 3: Verification (15 minutes)

1. **Download trained models**:
   ```bash
   aws s3 cp s3://crpbot-sagemaker-training/models/ ./models/v7_sagemaker/ --recursive
   ```

2. **Run diagnostic**:
   ```bash
   uv run python scripts/diagnose_v7_model.py
   ```

3. **Expected Results**:
   ```
   BTC-USD: âœ… PASS
     - Logit range: Â±8.3 (not Â±158,000!)
     - Overconfidence (>99%): 3.2% (not 100%!)
     - Scaler present: âœ… Yes
     - Test accuracy: 71.2%

   ETH-USD: âœ… PASS
     - Logit range: Â±7.9
     - Overconfidence (>99%): 4.1%
     - Scaler present: âœ… Yes
     - Test accuracy: 69.8%

   SOL-USD: âœ… PASS
     - Logit range: Â±9.1
     - Overconfidence (>99%): 5.3%
     - Scaler present: âœ… Yes
     - Test accuracy: 70.5%
   ```

### Phase 4: Deployment (15 minutes)

1. **Promote to production**:
   ```bash
   cp models/v7_sagemaker/*.pt models/promoted/
   ```

2. **Deploy to cloud server**:
   ```bash
   scp models/promoted/lstm_*_v7_enhanced.pt root@178.156.136.185:~/crpbot/models/promoted/
   ```

3. **Restart runtime**:
   ```bash
   ssh root@178.156.136.185 "cd ~/crpbot && pkill -f 'apps/runtime/main.py'"
   ssh root@178.156.136.185 "cd ~/crpbot && nohup .venv/bin/python3 apps/runtime/main.py --mode live > /tmp/v7_live.log 2>&1 &"
   ```

4. **Monitor first signals**:
   ```bash
   ssh root@178.156.136.185 "tail -f /tmp/v7_live.log"
   ```

### Expected Signal Output (With V7 Models)

```
ğŸ¯ SIGNAL #1234
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“Š BTC-USD LONG @ 68.3% Confidence  â† Real confidence! (not 100%)

ğŸ“ ENTRY ZONE
  Current Price: $79,568.14 âœ…
  Entry Range:   $78,978.26 - $79,568.14

ğŸ¯ ORDER TYPE: LIMIT BUY ğŸ’°
  Entry Price:   $78,978.26

ğŸ›¡ï¸ STOP LOSS
  Type: STOP @ $77,793.39
  Distance: 1.5% from entry

ğŸ¯ TAKE PROFIT
  Type: LIMIT SELL @ $81,112.45
  Distance: 2.7% from entry
  Risk:Reward: 1:1.8

ğŸ’° POSITION SIZING
  Account Balance: $10,000
  Risk per Trade:  $100 (1.0%)
  Position Size:   0.127 BTC
  Leverage:        10x

â° Signal Expires: 2025-11-16 15:35:00 EST (in 30 min)

ğŸ“ REASONING
  â€¢ Strong upward momentum (RSI: 64.2)
  â€¢ Price holding above EMA20 support
  â€¢ Volume confirming trend
```

---

## Cost Breakdown

### Training Costs

| Model | Instance | Training Time | Cost |
|-------|----------|---------------|------|
| BTC-USD | ml.g5.xlarge | 2-3 hours | ~$4.23 |
| ETH-USD | ml.g5.xlarge | 2-3 hours | ~$4.23 |
| SOL-USD | ml.g5.xlarge | 2-3 hours | ~$4.23 |

**Total Training**: ~$12-14

### Storage Costs (Negligible)

- S3 storage (training data): ~100 MB = $0.002/month
- S3 storage (models): ~1 MB = $0.00002/month

**Total Monthly**: <$0.01

---

## Critical Success Factors

### Before Training Starts

- [ ] S3 bucket created and accessible
- [ ] Training data uploaded (all 3 symbols)
- [ ] Training script includes scaler integration
- [ ] Diagnostic verification built into training loop
- [ ] SageMaker IAM role has S3 access

### During Training

- [ ] CloudWatch logs show scaler fitted
- [ ] Validation logits <20 after epoch 1
- [ ] Model checkpoint includes scaler
- [ ] Early stopping working correctly

### After Training

- [ ] All 3 models pass diagnostic
- [ ] Scaler present in all checkpoints
- [ ] Test accuracy â‰¥68%
- [ ] Confidence distribution realistic (60-85%)

---

## Files Created

### Documentation
- âœ… `/home/numan/crpbot/docs/SAGEMAKER_TRAINING_SETUP.md` - Full SageMaker setup guide
- âœ… `/home/numan/crpbot/V7_SAGEMAKER_MIGRATION_PLAN.md` - This document
- âœ… `/home/numan/crpbot/scripts/diagnose_v7_model.py` - Model diagnostic tool

### Previous Diagnostic Results
- âŒ `/home/numan/crpbot/reports/v6_model_diagnostic.json` - V6 failures
- âŒ `/tmp/v7_diagnostic.log` - V7 failures

### Model Files (Failed)
- âŒ `/home/numan/crpbot/models/v6_enhanced/*.pt` - V6 models (100% overconfident)
- âŒ `/home/numan/crpbot/models/v7_enhanced/*.pt` - V7 models (missing scaler)

---

## Next Steps

1. **Immediate**: Set up SageMaker environment and S3 bucket
2. **Training**: Launch 3 parallel training jobs on ml.g5.xlarge
3. **Verification**: Run diagnostic to confirm quality gates
4. **Deployment**: Promote and deploy to production
5. **Monitoring**: Watch first signals for realistic confidence

**Expected Timeline**:
- Setup: 30 minutes
- Training: 6-9 hours (parallel)
- Deployment: 15 minutes
- **Total**: ~10 hours to production-ready V7 models

---

## References

- **SageMaker Setup Guide**: `/home/numan/crpbot/docs/SAGEMAKER_TRAINING_SETUP.md`
- **V6 Diagnostic Report**: `/home/numan/crpbot/V6_DIAGNOSTIC_AND_V7_PLAN.md`
- **AWS GPU Training (EC2)**: `/home/numan/crpbot/docs/AWS_GPU_TRAINING_SETUP.md`
- **Signal Format**: `/home/numan/crpbot/apps/runtime/signal_formatter.py`

---

## Summary

**V6 Models**: Broken (100% overconfident, Â±40,000 logits)
**V7 Models (Current)**: Broken (missing scaler, Â±158,000 logits)
**V7 Models (SageMaker)**: Fix guaranteed with proper normalization

**Investment**: ~$12-14, 10 hours
**Outcome**: Production-ready models with realistic 60-85% confidence

**All infrastructure ready. Awaiting SageMaker training execution.**
