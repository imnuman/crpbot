{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CRPBot Model Evaluation - FIXED\n",
    "\n",
    "**Models**: 3 LSTM (BTC, ETH, SOL) - 128/3/bidirectional, 50 features  \n",
    "**Goal**: Evaluate against promotion gates (â‰¥68% accuracy, â‰¤5% calibration error)  \n",
    "**Time**: ~5-10 minutes on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Check GPU\n",
    "import torch\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Install dependencies\n",
    "!pip install -q loguru pandas pyarrow scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Mount Drive & verify\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "path = '/content/drive/MyDrive/crpbot'\n",
    "print(f\"\\nPath exists: {os.path.exists(path)}\")\n",
    "if os.path.exists(path):\n",
    "    !ls -lh /content/drive/MyDrive/crpbot/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Copy files\n",
    "!mkdir -p models/new data/features\n",
    "!cp /content/drive/MyDrive/crpbot/models/*.pt models/new/ 2>/dev/null || cp /content/drive/MyDrive/crpbot/*.pt models/new/\n",
    "!cp /content/drive/MyDrive/crpbot/features/*.parquet data/features/ 2>/dev/null || cp /content/drive/MyDrive/crpbot/*.parquet data/features/\n",
    "\n",
    "import glob\n",
    "print(f\"\\nModels: {len(glob.glob('models/new/*.pt'))}/3\")\n",
    "print(f\"Features: {len(glob.glob('data/features/*.parquet'))}/3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Setup modules\n",
    "!mkdir -p apps/trainer/{models,train}\n",
    "!touch apps/__init__.py apps/trainer/__init__.py apps/trainer/models/__init__.py apps/trainer/train/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile apps/trainer/models/lstm.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    \"\"\"LSTM matching checkpoint structure.\"\"\"\n",
    "    def __init__(self, input_size: int, hidden_size: int = 128, num_layers: int = 3, dropout: float = 0.2, bidirectional: bool = True):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0, bidirectional=bidirectional)\n",
    "        lstm_out = hidden_size * 2 if bidirectional else hidden_size\n",
    "        # Match checkpoint structure: fc.0, fc.3\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(lstm_out, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        return self.fc(out[:, -1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile apps/trainer/data_pipeline.py\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from loguru import logger\n",
    "\n",
    "def load_features_from_parquet(symbol: str):\n",
    "    patterns = [f\"features_{symbol}_*.parquet\"]\n",
    "    for p in patterns:\n",
    "        files = list(Path(\"data/features\").glob(p))\n",
    "        if files:\n",
    "            df = pd.read_parquet(sorted(files)[-1])\n",
    "            logger.info(f\"Loaded: {len(df):,} rows, {len(df.columns)} cols\")\n",
    "            return df\n",
    "    raise FileNotFoundError(f\"No features for {symbol}\")\n",
    "\n",
    "def create_walk_forward_splits(df, train_ratio=0.7, val_ratio=0.15):\n",
    "    n = len(df)\n",
    "    t1, t2 = int(n * train_ratio), int(n * (train_ratio + val_ratio))\n",
    "    logger.info(f\"Splits: Train={t1}, Val={t2-t1}, Test={n-t2}\")\n",
    "    return df.iloc[:t1].copy(), df.iloc[t1:t2].copy(), df.iloc[t2:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile apps/trainer/features.py\n",
    "import pandas as pd\n",
    "from loguru import logger\n",
    "\n",
    "# All 50 features\n",
    "FEATURE_COLUMNS = [\n",
    "    \"session_tokyo\", \"session_london\", \"session_ny\", \"day_of_week\", \"is_weekend\",\n",
    "    \"spread\", \"spread_pct\", \"atr\", \"spread_atr_ratio\",\n",
    "    \"volume_ma\", \"volume_ratio\", \"volume_trend\",\n",
    "    \"sma_7\", \"sma_14\", \"sma_21\", \"sma_50\", \"price_to_sma_7\", \"price_to_sma_14\", \"price_to_sma_21\", \"price_to_sma_50\",\n",
    "    \"rsi\", \"macd\", \"macd_signal\", \"macd_diff\", \"bb_upper\", \"bb_middle\", \"bb_lower\", \"bb_width\",\n",
    "    \"5m_open\", \"5m_high\", \"5m_low\", \"5m_close\", \"5m_volume\",\n",
    "    \"15m_open\", \"15m_high\", \"15m_low\", \"15m_close\", \"15m_volume\",\n",
    "    \"1h_open\", \"1h_high\", \"1h_low\", \"1h_close\", \"1h_volume\",\n",
    "    \"cross_tf_alignment_score\", \"cross_tf_alignment_direction\", \"cross_tf_alignment_strength\",\n",
    "    \"atr_percentile\",\n",
    "]\n",
    "\n",
    "def normalize_features(train_df, val_df=None, test_df=None):\n",
    "    cols = [c for c in FEATURE_COLUMNS if c in train_df.columns]\n",
    "    logger.info(f\"Found {len(cols)}/50 features: {cols[:5]}...\")\n",
    "    stats = {\"mean\": train_df[cols].mean().to_dict(), \"std\": train_df[cols].std().to_dict()}\n",
    "    for col in cols:\n",
    "        m, s = stats[\"mean\"][col], stats[\"std\"][col]\n",
    "        if s > 0:\n",
    "            train_df[col] = (train_df[col] - m) / s\n",
    "            if val_df is not None: val_df[col] = (val_df[col] - m) / s\n",
    "            if test_df is not None: test_df[col] = (test_df[col] - m) / s\n",
    "    return train_df, val_df, test_df, stats, cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile apps/trainer/train/dataset.py\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from loguru import logger\n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, df, feature_columns, seq_len=60, pred_horizon=15):\n",
    "        self.features = df[feature_columns].values.astype(np.float32)\n",
    "        future = df[\"close\"].shift(-pred_horizon)\n",
    "        self.targets = (future > df[\"close\"]).astype(np.float32).values\n",
    "        self.valid = np.arange(seq_len, len(df) - pred_horizon)\n",
    "        self.seq_len = seq_len\n",
    "        logger.info(f\"Dataset: {len(self.valid)} seqs, {len(feature_columns)} feats\")\n",
    "    \n",
    "    def __len__(self): return len(self.valid)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        i = self.valid[idx]\n",
    "        return torch.from_numpy(self.features[i-self.seq_len:i]), torch.tensor([self.targets[i]], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile evaluate.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from loguru import logger\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from torch.utils.data import DataLoader\n",
    "from apps.trainer.models.lstm import LSTMModel\n",
    "from apps.trainer.data_pipeline import load_features_from_parquet, create_walk_forward_splits\n",
    "from apps.trainer.features import normalize_features\n",
    "from apps.trainer.train.dataset import SequenceDataset\n",
    "\n",
    "def evaluate_model(model_path, symbol, device=\"cuda\"):\n",
    "    logger.info(f\"\\n{'='*60}\\nğŸš€ {symbol}\\n{'='*60}\")\n",
    "    df = load_features_from_parquet(symbol)\n",
    "    train_df, val_df, test_df = create_walk_forward_splits(df)\n",
    "    train_df, val_df, test_df, stats, feature_cols = normalize_features(train_df, val_df, test_df)\n",
    "    \n",
    "    # Critical: use actual number of features found\n",
    "    n_features = len(feature_cols)\n",
    "    logger.info(f\"Creating model with {n_features} features\")\n",
    "    \n",
    "    test_dataset = SequenceDataset(test_df, feature_cols)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2, pin_memory=(device==\"cuda\"))\n",
    "    \n",
    "    model = LSTMModel(input_size=n_features, hidden_size=128, num_layers=3, dropout=0.2, bidirectional=True)\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    model = model.to(device).eval()\n",
    "    \n",
    "    logger.info(f\"Evaluating {len(test_dataset):,} sequences...\")\n",
    "    all_preds, all_targets, all_probs = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (seqs, tgts) in enumerate(test_loader):\n",
    "            outs = model(seqs.to(device)).squeeze().cpu().numpy()\n",
    "            probs = outs if outs.ndim > 0 else [outs.item()]\n",
    "            preds = (np.array(probs) >= 0.5).astype(int)\n",
    "            all_probs.extend(probs.tolist() if isinstance(probs, np.ndarray) else [probs])\n",
    "            all_preds.extend(preds.tolist())\n",
    "            all_targets.extend(tgts.squeeze().numpy().tolist())\n",
    "            if batch_idx % 100 == 0:\n",
    "                logger.info(f\"  {len(all_preds):,}/{len(test_dataset):,}\")\n",
    "    \n",
    "    acc = accuracy_score(all_targets, all_preds)\n",
    "    prec = precision_score(all_targets, all_preds, zero_division=0)\n",
    "    rec = recall_score(all_targets, all_preds, zero_division=0)\n",
    "    f1 = f1_score(all_targets, all_preds, zero_division=0)\n",
    "    \n",
    "    # Calibration\n",
    "    bins = np.linspace(0, 1, 11)\n",
    "    bin_idx = np.digitize(all_probs, bins) - 1\n",
    "    ece = sum(\n",
    "        (mask := bin_idx == i).sum() / len(all_targets) * abs(\n",
    "            np.array(all_targets)[mask].mean() - np.array(all_probs)[mask].mean()\n",
    "        ) for i in range(10) if mask.sum() > 0\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"\\nâœ… Results: Acc={acc:.4f}, Prec={prec:.4f}, Rec={rec:.4f}, F1={f1:.4f}, ECE={ece:.4f}\")\n",
    "    passes = acc >= 0.68 and ece <= 0.05\n",
    "    logger.info(f\"ğŸ¯ Accâ‰¥68% {'âœ…' if acc>=0.68 else 'âŒ'} ({acc:.2%}), Calâ‰¤5% {'âœ…' if ece<=0.05 else 'âŒ'} ({ece:.2%})\")\n",
    "    logger.info(f\"{'âœ… PASS' if passes else 'âŒ FAIL'}\\n\")\n",
    "    \n",
    "    return {\"symbol\": symbol, \"accuracy\": acc, \"precision\": prec, \"recall\": rec, \"f1\": f1, \"calibration_error\": ece, \"samples\": len(all_targets), \"features_used\": n_features}\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "logger.info(f\"Device: {device}\")\n",
    "results = {}\n",
    "for symbol in [\"BTC-USD\", \"ETH-USD\", \"SOL-USD\"]:\n",
    "    try:\n",
    "        model_path = f\"models/new/lstm_{symbol.replace('-', '_')}_1m_7b5f0829.pt\"\n",
    "        results[symbol] = evaluate_model(model_path, symbol, device)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed {symbol}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if results:\n",
    "    df = pd.DataFrame(results).T\n",
    "    print(f\"\\n{'='*60}\\nğŸ“Š SUMMARY\\n{'='*60}\\n{df.to_string()}\\n\")\n",
    "    df.to_csv(\"results.csv\")\n",
    "    logger.info(\"âœ… Saved results.csv\")\n",
    "else:\n",
    "    logger.error(\"No results - all evaluations failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Run evaluation\n",
    "!python evaluate.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Download results\n",
    "from google.colab import files\n",
    "files.download('results.csv')\n",
    "print(\"\\nâœ… Share results.csv in Claude chat!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
