{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU-Accelerated Model Evaluation (50 Features)\n",
    "\n",
    "This notebook evaluates the 3 LSTM models (BTC, ETH, SOL) with 50-feature datasets on GPU.\n",
    "\n",
    "**Models**: `lstm_{SYMBOL}_USD_1m_7b5f0829.pt` (128/3/bidirectional, 1M+ params)\n",
    "\n",
    "**Setup**:\n",
    "1. Runtime â†’ Change runtime type â†’ GPU (T4)\n",
    "2. Upload model files and feature datasets\n",
    "3. Run all cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q loguru pandas pyarrow scikit-learn numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory structure\n",
    "!mkdir -p models/new data/features scripts apps/trainer/{models,train,eval} libs/{constants,rl_env}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option A: Upload Files from Local Machine\n",
    "\n",
    "Upload these files using the Colab file browser (left sidebar):\n",
    "\n",
    "**Models** (3.9 MB each â†’ `models/new/`):\n",
    "- `lstm_BTC_USD_1m_7b5f0829.pt`\n",
    "- `lstm_ETH_USD_1m_7b5f0829.pt`\n",
    "- `lstm_SOL_USD_1m_7b5f0829.pt`\n",
    "\n",
    "**Feature Datasets** (200+ MB each â†’ `data/features/`):\n",
    "- `features_BTC-USD_1m_2025-11-13_50feat.parquet`\n",
    "- `features_ETH-USD_1m_2025-11-13_50feat.parquet`\n",
    "- `features_SOL-USD_1m_2025-11-13_50feat.parquet`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option B: Download from Google Drive\n",
    "\n",
    "If you've uploaded files to Google Drive, mount it and copy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to use Google Drive\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# Copy files from Google Drive\n",
    "# !cp /content/drive/MyDrive/crpbot/models/new/*.pt models/new/\n",
    "# !cp /content/drive/MyDrive/crpbot/data/features/*_50feat.parquet data/features/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify files uploaded\n",
    "!ls -lh models/new/\n",
    "!ls -lh data/features/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Code Modules\n",
    "\n",
    "We need to recreate the necessary Python modules for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile apps/trainer/models/lstm.py\n",
    "\"\"\"LSTM model for direction prediction.\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    \"\"\"Bidirectional LSTM for binary classification.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        hidden_size: int = 128,\n",
    "        num_layers: int = 3,\n",
    "        dropout: float = 0.2,\n",
    "        bidirectional: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size,\n",
    "            hidden_size,\n",
    "            num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=bidirectional,\n",
    "        )\n",
    "\n",
    "        lstm_output_size = hidden_size * 2 if bidirectional else hidden_size\n",
    "        self.fc1 = nn.Linear(lstm_output_size, 64)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            x: (batch_size, seq_len, input_size)\n",
    "            \n",
    "        Returns:\n",
    "            Predictions: (batch_size, 1)\n",
    "        \"\"\"\n",
    "        lstm_out, _ = self.lstm(x)  # (batch, seq, hidden*2)\n",
    "        last_output = lstm_out[:, -1, :]  # (batch, hidden*2)\n",
    "        x = torch.relu(self.fc1(last_output))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return self.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile apps/trainer/data_pipeline.py\n",
    "\"\"\"Data loading and preprocessing.\"\"\"\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from loguru import logger\n",
    "\n",
    "def load_features_from_parquet(symbol: str, features_dir: str = \"data/features\") -> pd.DataFrame:\n",
    "    \"\"\"Load features from parquet.\"\"\"\n",
    "    features_path = Path(features_dir) / f\"features_{symbol}_1m_latest.parquet\"\n",
    "    if not features_path.exists():\n",
    "        # Try dated file\n",
    "        dated_files = list(Path(features_dir).glob(f\"features_{symbol}_1m_*_50feat.parquet\"))\n",
    "        if dated_files:\n",
    "            features_path = sorted(dated_files)[-1]  # Use latest\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"No feature file found for {symbol}\")\n",
    "    \n",
    "    df = pd.read_parquet(features_path)\n",
    "    logger.info(f\"Loaded features from {features_path}: {len(df):,} rows, {len(df.columns)} columns\")\n",
    "    return df\n",
    "\n",
    "def create_walk_forward_splits(\n",
    "    df: pd.DataFrame,\n",
    "    train_ratio: float = 0.7,\n",
    "    val_ratio: float = 0.15,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Split data chronologically.\"\"\"\n",
    "    n = len(df)\n",
    "    train_end = int(n * train_ratio)\n",
    "    val_end = int(n * (train_ratio + val_ratio))\n",
    "    \n",
    "    train_df = df.iloc[:train_end].copy()\n",
    "    val_df = df.iloc[train_end:val_end].copy()\n",
    "    test_df = df.iloc[val_end:].copy()\n",
    "    \n",
    "    logger.info(f\"Creating walk-forward splits: train until {train_df['timestamp'].max()}, val until {val_df['timestamp'].max()}\")\n",
    "    logger.info(f\"Split sizes: Train={len(train_df)}, Val={len(val_df)}, Test={len(test_df)}\")\n",
    "    \n",
    "    return train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile apps/trainer/features.py\n",
    "\"\"\"Feature normalization.\"\"\"\n",
    "from typing import List, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from loguru import logger\n",
    "\n",
    "FEATURE_COLUMNS = [\n",
    "    # Session features (5)\n",
    "    \"session_tokyo\", \"session_london\", \"session_ny\", \"day_of_week\", \"is_weekend\",\n",
    "    # Spread features (4)\n",
    "    \"spread\", \"spread_pct\", \"atr\", \"spread_atr_ratio\",\n",
    "    # Volume features (3)\n",
    "    \"volume_ma\", \"volume_ratio\", \"volume_trend\",\n",
    "    # Moving averages (8)\n",
    "    \"sma_7\", \"sma_14\", \"sma_21\", \"sma_50\",\n",
    "    \"price_to_sma_7\", \"price_to_sma_14\", \"price_to_sma_21\", \"price_to_sma_50\",\n",
    "    # Technical indicators (8)\n",
    "    \"rsi\", \"macd\", \"macd_signal\", \"macd_diff\",\n",
    "    \"bb_upper\", \"bb_middle\", \"bb_lower\", \"bb_width\",\n",
    "    # Multi-TF OHLCV (15)\n",
    "    \"5m_open\", \"5m_high\", \"5m_low\", \"5m_close\", \"5m_volume\",\n",
    "    \"15m_open\", \"15m_high\", \"15m_low\", \"15m_close\", \"15m_volume\",\n",
    "    \"1h_open\", \"1h_high\", \"1h_low\", \"1h_close\", \"1h_volume\",\n",
    "    # Cross-TF alignment (3)\n",
    "    \"cross_tf_alignment_score\", \"cross_tf_alignment_direction\", \"cross_tf_alignment_strength\",\n",
    "    # Volatility (1)\n",
    "    \"atr_percentile\",\n",
    "]\n",
    "\n",
    "def normalize_features(\n",
    "    train_df: pd.DataFrame,\n",
    "    val_df: pd.DataFrame = None,\n",
    "    test_df: pd.DataFrame = None,\n",
    "    method: str = \"standard\",\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, dict]:\n",
    "    \"\"\"Normalize features using training set statistics.\"\"\"\n",
    "    feature_cols = [c for c in FEATURE_COLUMNS if c in train_df.columns]\n",
    "    \n",
    "    if method == \"standard\":\n",
    "        stats = {\n",
    "            \"mean\": train_df[feature_cols].mean().to_dict(),\n",
    "            \"std\": train_df[feature_cols].std().to_dict(),\n",
    "        }\n",
    "        \n",
    "        for col in feature_cols:\n",
    "            mean, std = stats[\"mean\"][col], stats[\"std\"][col]\n",
    "            if std > 0:\n",
    "                train_df[col] = (train_df[col] - mean) / std\n",
    "                if val_df is not None:\n",
    "                    val_df[col] = (val_df[col] - mean) / std\n",
    "                if test_df is not None:\n",
    "                    test_df[col] = (test_df[col] - mean) / std\n",
    "    \n",
    "    logger.info(f\"Normalized {len(feature_cols)} features using {method} method\")\n",
    "    return train_df, val_df, test_df, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile apps/trainer/train/dataset.py\n",
    "\"\"\"PyTorch dataset for sequences.\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from loguru import logger\n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "    \"\"\"Dataset for time series sequences.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        feature_columns: list[str],\n",
    "        sequence_length: int = 60,\n",
    "        prediction_horizon: int = 15,\n",
    "        prediction_type: str = \"direction\",\n",
    "    ):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.feature_columns = feature_columns\n",
    "        self.sequence_length = sequence_length\n",
    "        self.prediction_horizon = prediction_horizon\n",
    "        self.prediction_type = prediction_type\n",
    "\n",
    "        # Precompute features and targets\n",
    "        self.features = self.df[feature_columns].values.astype(np.float32)\n",
    "        self._precompute_targets()\n",
    "\n",
    "        # Valid indices (enough history + future)\n",
    "        self.valid_indices = np.arange(\n",
    "            sequence_length,\n",
    "            len(self.df) - prediction_horizon\n",
    "        )\n",
    "        \n",
    "        logger.info(\n",
    "            f\"Created dataset: {len(self.valid_indices)} sequences, \"\n",
    "            f\"{len(feature_columns)} features, prediction_type={prediction_type}\"\n",
    "        )\n",
    "\n",
    "    def _precompute_targets(self):\n",
    "        \"\"\"Precompute target values.\"\"\"\n",
    "        future_close = self.df[\"close\"].shift(-self.prediction_horizon)\n",
    "        current_close = self.df[\"close\"]\n",
    "        \n",
    "        if self.prediction_type == \"direction\":\n",
    "            self.targets = (future_close > current_close).astype(np.float32).values\n",
    "        else:\n",
    "            self.targets = ((future_close - current_close) / current_close).astype(np.float32).values\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.valid_indices)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        actual_idx = self.valid_indices[idx]\n",
    "        \n",
    "        # Get sequence\n",
    "        seq_start = actual_idx - self.sequence_length\n",
    "        seq_end = actual_idx\n",
    "        sequence = self.features[seq_start:seq_end]\n",
    "        \n",
    "        target = self.targets[actual_idx]\n",
    "        \n",
    "        return torch.from_numpy(sequence), torch.tensor([target], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile evaluate_gpu.py\n",
    "\"\"\"GPU-accelerated model evaluation.\"\"\"\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from loguru import logger\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Add modules to path\n",
    "sys.path.insert(0, '/content')\n",
    "\n",
    "from apps.trainer.models.lstm import LSTMModel\n",
    "from apps.trainer.data_pipeline import load_features_from_parquet, create_walk_forward_splits\n",
    "from apps.trainer.features import normalize_features, FEATURE_COLUMNS\n",
    "from apps.trainer.train.dataset import SequenceDataset\n",
    "\n",
    "def evaluate_model(\n",
    "    model_path: str,\n",
    "    symbol: str,\n",
    "    device: str = \"cuda\",\n",
    "    batch_size: int = 64,\n",
    ") -> Dict:\n",
    "    \"\"\"Evaluate model on GPU.\"\"\"\n",
    "    logger.info(f\"ðŸš€ Evaluating {symbol} on {device.upper()}\")\n",
    "    \n",
    "    # Load data\n",
    "    df = load_features_from_parquet(symbol)\n",
    "    train_df, val_df, test_df = create_walk_forward_splits(df)\n",
    "    \n",
    "    # Normalize\n",
    "    train_df, val_df, test_df, stats = normalize_features(train_df, val_df, test_df)\n",
    "    \n",
    "    # Create dataset\n",
    "    feature_cols = [c for c in FEATURE_COLUMNS if c in test_df.columns]\n",
    "    test_dataset = SequenceDataset(\n",
    "        test_df,\n",
    "        feature_columns=feature_cols,\n",
    "        sequence_length=60,\n",
    "        prediction_horizon=15,\n",
    "        prediction_type=\"direction\",\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=2,\n",
    "        pin_memory=True if device == \"cuda\" else False,\n",
    "    )\n",
    "    \n",
    "    # Load model\n",
    "    model = LSTMModel(\n",
    "        input_size=len(feature_cols),\n",
    "        hidden_size=128,\n",
    "        num_layers=3,\n",
    "        dropout=0.2,\n",
    "        bidirectional=True,\n",
    "    )\n",
    "    \n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    logger.info(f\"Evaluating on {len(test_dataset):,} sequences...\")\n",
    "    \n",
    "    # Evaluate\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (sequences, targets) in enumerate(test_loader):\n",
    "            sequences = sequences.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            outputs = model(sequences)\n",
    "            probs = outputs.squeeze().cpu().numpy()\n",
    "            preds = (probs >= 0.5).astype(int)\n",
    "            \n",
    "            all_probs.extend(probs.tolist() if probs.ndim > 0 else [probs.item()])\n",
    "            all_preds.extend(preds.tolist() if preds.ndim > 0 else [preds.item()])\n",
    "            all_targets.extend(targets.squeeze().cpu().numpy().tolist())\n",
    "            \n",
    "            if batch_idx % 100 == 0:\n",
    "                progress = (batch_idx + 1) * batch_size\n",
    "                total = len(test_dataset)\n",
    "                logger.info(f\"  Progress: {progress:,}/{total:,} ({100*progress/total:.1f}%)\")\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_targets, all_preds)\n",
    "    precision = precision_score(all_targets, all_preds, zero_division=0)\n",
    "    recall = recall_score(all_targets, all_preds, zero_division=0)\n",
    "    f1 = f1_score(all_targets, all_preds, zero_division=0)\n",
    "    \n",
    "    # Calibration error (simplified ECE)\n",
    "    bins = np.linspace(0, 1, 11)\n",
    "    bin_indices = np.digitize(all_probs, bins) - 1\n",
    "    ece = 0.0\n",
    "    for i in range(10):\n",
    "        mask = bin_indices == i\n",
    "        if mask.sum() > 0:\n",
    "            bin_accuracy = np.array(all_targets)[mask].mean()\n",
    "            bin_confidence = np.array(all_probs)[mask].mean()\n",
    "            ece += mask.sum() / len(all_targets) * abs(bin_accuracy - bin_confidence)\n",
    "    \n",
    "    results = {\n",
    "        \"symbol\": symbol,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"calibration_error\": ece,\n",
    "        \"num_samples\": len(all_targets),\n",
    "    }\n",
    "    \n",
    "    logger.info(f\"âœ… {symbol} Results:\")\n",
    "    logger.info(f\"  Accuracy: {accuracy:.4f}\")\n",
    "    logger.info(f\"  Precision: {precision:.4f}\")\n",
    "    logger.info(f\"  Recall: {recall:.4f}\")\n",
    "    logger.info(f\"  F1: {f1:.4f}\")\n",
    "    logger.info(f\"  Calibration Error: {ece:.4f}\")\n",
    "    \n",
    "    # Check promotion gates\n",
    "    passes_accuracy = accuracy >= 0.68\n",
    "    passes_calibration = ece <= 0.05\n",
    "    \n",
    "    logger.info(f\"\\nðŸŽ¯ Promotion Gates:\")\n",
    "    logger.info(f\"  Accuracy â‰¥ 68%: {'âœ… PASS' if passes_accuracy else 'âŒ FAIL'} ({accuracy:.2%})\")\n",
    "    logger.info(f\"  Calibration â‰¤ 5%: {'âœ… PASS' if passes_calibration else 'âŒ FAIL'} ({ece:.2%})\")\n",
    "    logger.info(f\"  Overall: {'âœ… PROMOTION APPROVED' if (passes_accuracy and passes_calibration) else 'âŒ NOT READY'}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    logger.info(f\"Using device: {device}\")\n",
    "    \n",
    "    symbols = [\"BTC-USD\", \"ETH-USD\", \"SOL-USD\"]\n",
    "    all_results = {}\n",
    "    \n",
    "    for symbol in symbols:\n",
    "        model_path = f\"models/new/lstm_{symbol.replace('-', '_')}_1m_7b5f0829.pt\"\n",
    "        logger.info(f\"\\n{'='*60}\")\n",
    "        logger.info(f\"Evaluating {symbol}\")\n",
    "        logger.info(f\"{'='*60}\")\n",
    "        \n",
    "        results = evaluate_model(model_path, symbol, device=device)\n",
    "        all_results[symbol] = results\n",
    "    \n",
    "    # Summary\n",
    "    logger.info(f\"\\n\\n{'='*60}\")\n",
    "    logger.info(\"ðŸ“Š EVALUATION SUMMARY\")\n",
    "    logger.info(f\"{'='*60}\")\n",
    "    \n",
    "    results_df = pd.DataFrame(all_results).T\n",
    "    print(results_df.to_string())\n",
    "    \n",
    "    # Save results\n",
    "    results_df.to_csv(\"evaluation_results.csv\")\n",
    "    logger.info(f\"\\nâœ… Results saved to evaluation_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Evaluation\n",
    "\n",
    "This will evaluate all 3 models on GPU in parallel batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python evaluate_gpu.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Results\n",
    "\n",
    "Download the evaluation results CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.download('evaluation_results.csv')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
