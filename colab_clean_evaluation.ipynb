{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CRPBot Model Evaluation - Clean Setup\n",
    "\n",
    "**Models**: 3 LSTM (BTC, ETH, SOL) - 128/3/bidirectional, 50 features  \n",
    "**Goal**: Evaluate against promotion gates (â‰¥68% accuracy, â‰¤5% calibration error)  \n",
    "**Time**: ~5-10 minutes on GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"âš ï¸ WARNING: GPU not available! Go to Runtime â†’ Change runtime type â†’ GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q loguru pandas pyarrow scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Verify files exist\n",
    "import os\n",
    "drive_path = '/content/drive/MyDrive/colab'\n",
    "\n",
    "print(\"\\nðŸ“‚ Checking files...\")\n",
    "print(f\"\\nDrive path exists: {os.path.exists(drive_path)}\")\n",
    "\n",
    "if os.path.exists(drive_path):\n",
    "    print(\"\\nContents:\")\n",
    "    !ls -lh /content/drive/MyDrive/colab/\n",
    "else:\n",
    "    print(\"âŒ ERROR: /content/drive/MyDrive/colab/ not found!\")\n",
    "    print(\"Please ensure your files are uploaded to 'My Drive/colab/' folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Copy Files to Colab Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories\n",
    "!mkdir -p models/new data/features\n",
    "\n",
    "# Copy model files\n",
    "print(\"ðŸ“¦ Copying model files...\")\n",
    "!cp /content/drive/MyDrive/colab/models/*.pt models/new/ 2>/dev/null || \\\n",
    " cp /content/drive/MyDrive/colab/*.pt models/new/ 2>/dev/null || \\\n",
    " echo \"âš ï¸ No .pt files found in expected locations\"\n",
    "\n",
    "# Copy feature files\n",
    "print(\"\\nðŸ“¦ Copying feature files...\")\n",
    "!cp /content/drive/MyDrive/colab/features/*.parquet data/features/ 2>/dev/null || \\\n",
    " cp /content/drive/MyDrive/colab/*.parquet data/features/ 2>/dev/null || \\\n",
    " echo \"âš ï¸ No .parquet files found in expected locations\"\n",
    "\n",
    "# Verify\n",
    "print(\"\\nâœ… Files in workspace:\")\n",
    "print(\"\\nModels:\")\n",
    "!ls -lh models/new/\n",
    "print(\"\\nFeatures:\")\n",
    "!ls -lh data/features/\n",
    "\n",
    "# Check counts\n",
    "import glob\n",
    "models = glob.glob('models/new/*.pt')\n",
    "features = glob.glob('data/features/*.parquet')\n",
    "\n",
    "print(f\"\\nðŸ“Š Summary:\")\n",
    "print(f\"  Models found: {len(models)}/3 expected\")\n",
    "print(f\"  Features found: {len(features)}/3 expected\")\n",
    "\n",
    "if len(models) != 3 or len(features) != 3:\n",
    "    print(\"\\nâš ï¸ WARNING: Missing files! Expected 3 models + 3 features\")\n",
    "    print(\"Please check your Google Drive folder structure.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create Code Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p apps/trainer/{models,train,eval} libs/{constants,rl_env}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile apps/trainer/models/lstm.py\n",
    "\"\"\"LSTM model for direction prediction.\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    \"\"\"Bidirectional LSTM for binary classification.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        hidden_size: int = 128,\n",
    "        num_layers: int = 3,\n",
    "        dropout: float = 0.2,\n",
    "        bidirectional: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size,\n",
    "            hidden_size,\n",
    "            num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=bidirectional,\n",
    "        )\n",
    "\n",
    "        lstm_output_size = hidden_size * 2 if bidirectional else hidden_size\n",
    "        self.fc1 = nn.Linear(lstm_output_size, 64)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        last_output = lstm_out[:, -1, :]\n",
    "        x = torch.relu(self.fc1(last_output))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return self.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile apps/trainer/data_pipeline.py\n",
    "\"\"\"Data loading and preprocessing.\"\"\"\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "\n",
    "import pandas as pd\n",
    "from loguru import logger\n",
    "\n",
    "def load_features_from_parquet(symbol: str, features_dir: str = \"data/features\") -> pd.DataFrame:\n",
    "    \"\"\"Load features from parquet.\"\"\"\n",
    "    # Try multiple patterns\n",
    "    patterns = [\n",
    "        f\"features_{symbol}_1m_latest.parquet\",\n",
    "        f\"features_{symbol}_1m_*_50feat.parquet\",\n",
    "        f\"features_{symbol}_*.parquet\",\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        files = list(Path(features_dir).glob(pattern))\n",
    "        if files:\n",
    "            features_path = sorted(files)[-1]  # Use latest\n",
    "            df = pd.read_parquet(features_path)\n",
    "            logger.info(f\"Loaded features from {features_path}: {len(df):,} rows, {len(df.columns)} columns\")\n",
    "            return df\n",
    "    \n",
    "    raise FileNotFoundError(f\"No feature file found for {symbol} in {features_dir}\")\n",
    "\n",
    "def create_walk_forward_splits(\n",
    "    df: pd.DataFrame,\n",
    "    train_ratio: float = 0.7,\n",
    "    val_ratio: float = 0.15,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Split data chronologically.\"\"\"\n",
    "    n = len(df)\n",
    "    train_end = int(n * train_ratio)\n",
    "    val_end = int(n * (train_ratio + val_ratio))\n",
    "    \n",
    "    train_df = df.iloc[:train_end].copy()\n",
    "    val_df = df.iloc[train_end:val_end].copy()\n",
    "    test_df = df.iloc[val_end:].copy()\n",
    "    \n",
    "    logger.info(f\"Creating walk-forward splits: train until {train_df['timestamp'].max()}, val until {val_df['timestamp'].max()}\")\n",
    "    logger.info(f\"Split sizes: Train={len(train_df)}, Val={len(val_df)}, Test={len(test_df)}\")\n",
    "    \n",
    "    return train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile apps/trainer/features.py\n",
    "\"\"\"Feature normalization.\"\"\"\n",
    "from typing import List, Tuple\n",
    "import pandas as pd\n",
    "from loguru import logger\n",
    "\n",
    "FEATURE_COLUMNS = [\n",
    "    # Session (5)\n",
    "    \"session_tokyo\", \"session_london\", \"session_ny\", \"day_of_week\", \"is_weekend\",\n",
    "    # Spread (4)\n",
    "    \"spread\", \"spread_pct\", \"atr\", \"spread_atr_ratio\",\n",
    "    # Volume (3)\n",
    "    \"volume_ma\", \"volume_ratio\", \"volume_trend\",\n",
    "    # MA (8)\n",
    "    \"sma_7\", \"sma_14\", \"sma_21\", \"sma_50\",\n",
    "    \"price_to_sma_7\", \"price_to_sma_14\", \"price_to_sma_21\", \"price_to_sma_50\",\n",
    "    # Technical (8)\n",
    "    \"rsi\", \"macd\", \"macd_signal\", \"macd_diff\",\n",
    "    \"bb_upper\", \"bb_middle\", \"bb_lower\", \"bb_width\",\n",
    "    # Multi-TF OHLCV (15)\n",
    "    \"5m_open\", \"5m_high\", \"5m_low\", \"5m_close\", \"5m_volume\",\n",
    "    \"15m_open\", \"15m_high\", \"15m_low\", \"15m_close\", \"15m_volume\",\n",
    "    \"1h_open\", \"1h_high\", \"1h_low\", \"1h_close\", \"1h_volume\",\n",
    "    # Cross-TF (3)\n",
    "    \"cross_tf_alignment_score\", \"cross_tf_alignment_direction\", \"cross_tf_alignment_strength\",\n",
    "    # Volatility (1)\n",
    "    \"atr_percentile\",\n",
    "]\n",
    "\n",
    "def normalize_features(\n",
    "    train_df: pd.DataFrame,\n",
    "    val_df: pd.DataFrame = None,\n",
    "    test_df: pd.DataFrame = None,\n",
    "    method: str = \"standard\",\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, dict]:\n",
    "    \"\"\"Normalize features.\"\"\"\n",
    "    feature_cols = [c for c in FEATURE_COLUMNS if c in train_df.columns]\n",
    "    \n",
    "    if method == \"standard\":\n",
    "        stats = {\n",
    "            \"mean\": train_df[feature_cols].mean().to_dict(),\n",
    "            \"std\": train_df[feature_cols].std().to_dict(),\n",
    "        }\n",
    "        \n",
    "        for col in feature_cols:\n",
    "            mean, std = stats[\"mean\"][col], stats[\"std\"][col]\n",
    "            if std > 0:\n",
    "                train_df[col] = (train_df[col] - mean) / std\n",
    "                if val_df is not None:\n",
    "                    val_df[col] = (val_df[col] - mean) / std\n",
    "                if test_df is not None:\n",
    "                    test_df[col] = (test_df[col] - mean) / std\n",
    "    \n",
    "    logger.info(f\"Normalized {len(feature_cols)} features using {method} method\")\n",
    "    return train_df, val_df, test_df, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile apps/trainer/train/dataset.py\n",
    "\"\"\"PyTorch dataset.\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from loguru import logger\n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        feature_columns: list[str],\n",
    "        sequence_length: int = 60,\n",
    "        prediction_horizon: int = 15,\n",
    "        prediction_type: str = \"direction\",\n",
    "    ):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.feature_columns = feature_columns\n",
    "        self.sequence_length = sequence_length\n",
    "        self.prediction_horizon = prediction_horizon\n",
    "        self.prediction_type = prediction_type\n",
    "\n",
    "        self.features = self.df[feature_columns].values.astype(np.float32)\n",
    "        self._precompute_targets()\n",
    "\n",
    "        self.valid_indices = np.arange(\n",
    "            sequence_length,\n",
    "            len(self.df) - prediction_horizon\n",
    "        )\n",
    "        \n",
    "        logger.info(\n",
    "            f\"Created dataset: {len(self.valid_indices)} sequences, \"\n",
    "            f\"{len(feature_columns)} features, prediction_type={prediction_type}\"\n",
    "        )\n",
    "\n",
    "    def _precompute_targets(self):\n",
    "        future_close = self.df[\"close\"].shift(-self.prediction_horizon)\n",
    "        current_close = self.df[\"close\"]\n",
    "        \n",
    "        if self.prediction_type == \"direction\":\n",
    "            self.targets = (future_close > current_close).astype(np.float32).values\n",
    "        else:\n",
    "            self.targets = ((future_close - current_close) / current_close).astype(np.float32).values\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.valid_indices)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        actual_idx = self.valid_indices[idx]\n",
    "        seq_start = actual_idx - self.sequence_length\n",
    "        seq_end = actual_idx\n",
    "        sequence = self.features[seq_start:seq_end]\n",
    "        target = self.targets[actual_idx]\n",
    "        return torch.from_numpy(sequence), torch.tensor([target], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile evaluate.py\n",
    "\"\"\"Evaluate all 3 models on GPU.\"\"\"\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from loguru import logger\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from apps.trainer.models.lstm import LSTMModel\n",
    "from apps.trainer.data_pipeline import load_features_from_parquet, create_walk_forward_splits\n",
    "from apps.trainer.features import normalize_features, FEATURE_COLUMNS\n",
    "from apps.trainer.train.dataset import SequenceDataset\n",
    "\n",
    "def evaluate_model(model_path: str, symbol: str, device: str = \"cuda\", batch_size: int = 64):\n",
    "    logger.info(f\"{'='*60}\")\n",
    "    logger.info(f\"ðŸš€ Evaluating {symbol}\")\n",
    "    logger.info(f\"{'='*60}\")\n",
    "    \n",
    "    # Load data\n",
    "    df = load_features_from_parquet(symbol)\n",
    "    train_df, val_df, test_df = create_walk_forward_splits(df)\n",
    "    train_df, val_df, test_df, stats = normalize_features(train_df, val_df, test_df)\n",
    "    \n",
    "    # Create dataset\n",
    "    feature_cols = [c for c in FEATURE_COLUMNS if c in test_df.columns]\n",
    "    test_dataset = SequenceDataset(\n",
    "        test_df, feature_columns=feature_cols,\n",
    "        sequence_length=60, prediction_horizon=15, prediction_type=\"direction\"\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, batch_size=batch_size, shuffle=False,\n",
    "        num_workers=2, pin_memory=(device == \"cuda\")\n",
    "    )\n",
    "    \n",
    "    # Load model\n",
    "    model = LSTMModel(\n",
    "        input_size=len(feature_cols),\n",
    "        hidden_size=128, num_layers=3,\n",
    "        dropout=0.2, bidirectional=True\n",
    "    )\n",
    "    \n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    logger.info(f\"Evaluating on {len(test_dataset):,} sequences...\")\n",
    "    \n",
    "    # Evaluate\n",
    "    all_preds, all_targets, all_probs = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (sequences, targets) in enumerate(test_loader):\n",
    "            sequences = sequences.to(device)\n",
    "            targets = targets.to(device)\n",
    "            outputs = model(sequences)\n",
    "            probs = outputs.squeeze().cpu().numpy()\n",
    "            preds = (probs >= 0.5).astype(int)\n",
    "            \n",
    "            all_probs.extend(probs.tolist() if probs.ndim > 0 else [probs.item()])\n",
    "            all_preds.extend(preds.tolist() if preds.ndim > 0 else [preds.item()])\n",
    "            all_targets.extend(targets.squeeze().cpu().numpy().tolist())\n",
    "            \n",
    "            if batch_idx % 100 == 0:\n",
    "                logger.info(f\"  Progress: {(batch_idx + 1) * batch_size:,}/{len(test_dataset):,}\")\n",
    "    \n",
    "    # Metrics\n",
    "    accuracy = accuracy_score(all_targets, all_preds)\n",
    "    precision = precision_score(all_targets, all_preds, zero_division=0)\n",
    "    recall = recall_score(all_targets, all_preds, zero_division=0)\n",
    "    f1 = f1_score(all_targets, all_preds, zero_division=0)\n",
    "    \n",
    "    # Calibration error\n",
    "    bins = np.linspace(0, 1, 11)\n",
    "    bin_indices = np.digitize(all_probs, bins) - 1\n",
    "    ece = 0.0\n",
    "    for i in range(10):\n",
    "        mask = bin_indices == i\n",
    "        if mask.sum() > 0:\n",
    "            bin_accuracy = np.array(all_targets)[mask].mean()\n",
    "            bin_confidence = np.array(all_probs)[mask].mean()\n",
    "            ece += mask.sum() / len(all_targets) * abs(bin_accuracy - bin_confidence)\n",
    "    \n",
    "    logger.info(f\"\\nâœ… {symbol} Results:\")\n",
    "    logger.info(f\"  Accuracy: {accuracy:.4f}\")\n",
    "    logger.info(f\"  Precision: {precision:.4f}\")\n",
    "    logger.info(f\"  Recall: {recall:.4f}\")\n",
    "    logger.info(f\"  F1: {f1:.4f}\")\n",
    "    logger.info(f\"  Calibration Error: {ece:.4f}\")\n",
    "    \n",
    "    passes_acc = accuracy >= 0.68\n",
    "    passes_cal = ece <= 0.05\n",
    "    \n",
    "    logger.info(f\"\\nðŸŽ¯ Promotion Gates:\")\n",
    "    logger.info(f\"  Accuracy â‰¥ 68%: {'âœ… PASS' if passes_acc else 'âŒ FAIL'} ({accuracy:.2%})\")\n",
    "    logger.info(f\"  Calibration â‰¤ 5%: {'âœ… PASS' if passes_cal else 'âŒ FAIL'} ({ece:.2%})\")\n",
    "    logger.info(f\"  Overall: {'âœ… PROMOTION APPROVED' if (passes_acc and passes_cal) else 'âŒ NOT READY'}\\n\")\n",
    "    \n",
    "    return {\n",
    "        \"symbol\": symbol,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"calibration_error\": ece,\n",
    "        \"num_samples\": len(all_targets),\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    logger.info(f\"Using device: {device}\")\n",
    "    \n",
    "    symbols = [\"BTC-USD\", \"ETH-USD\", \"SOL-USD\"]\n",
    "    all_results = {}\n",
    "    \n",
    "    for symbol in symbols:\n",
    "        model_path = f\"models/new/lstm_{symbol.replace('-', '_')}_1m_7b5f0829.pt\"\n",
    "        results = evaluate_model(model_path, symbol, device=device)\n",
    "        all_results[symbol] = results\n",
    "    \n",
    "    logger.info(f\"\\n\\n{'='*60}\")\n",
    "    logger.info(\"ðŸ“Š EVALUATION SUMMARY\")\n",
    "    logger.info(f\"{'='*60}\")\n",
    "    \n",
    "    results_df = pd.DataFrame(all_results).T\n",
    "    print(results_df.to_string())\n",
    "    \n",
    "    results_df.to_csv(\"evaluation_results.csv\")\n",
    "    logger.info(f\"\\nâœ… Results saved to evaluation_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation\n",
    "!python evaluate.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.download('evaluation_results.csv')\n",
    "\n",
    "# Also display\n",
    "import pandas as pd\n",
    "results = pd.read_csv('evaluation_results.csv', index_col=0)\n",
    "print(\"\\nðŸ“Š Final Results:\")\n",
    "print(results.to_string())"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
